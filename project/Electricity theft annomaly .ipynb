{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6651cc7a-d2ea-4053-ad53-62223761c6b6",
   "metadata": {},
   "source": [
    "In this notebook, we going to create an electricity anomaly detection system to combat household electricity theft\n",
    "\n",
    "Since it was hard to find datasets with the data required we created a synthetic database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c1a94-7a27-4e7a-81c9-9cbfcbb62204",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.12.4)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#packages and dependencies \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8886f3-6f93-4f36-9364-b2660fc46277",
   "metadata": {},
   "source": [
    "# 1.  Generate the Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2755883f-7bb1-4694-8c31-13559c1acf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a date range (four years of daily data)\n",
    "date_range = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')\n",
    "\n",
    "# Number of households to achieve approximately 500,000 records (500,000 / 1461 ≈ 342 households)\n",
    "n_households = 342\n",
    "\n",
    "# Simulate number of people per household (randomly assigning between 1 and 8 people per household)\n",
    "household_size = np.random.randint(1, 9, size=n_households)\n",
    "\n",
    "# Base consumption is now a function of household size (larger households consume more on average)\n",
    "# Use base consumption between 10 kWh (small households) to 30 kWh (large households)\n",
    "base_consumption = 10 + household_size * np.random.uniform(low=1, high=3, size=n_households)\n",
    "\n",
    "# Create an empty dictionary to hold the consumption data\n",
    "consumption_data = {}\n",
    "\n",
    "for i in range(n_households):\n",
    "    # Simulate normal consumption with random daily variations\n",
    "    normal_usage = base_consumption[i] + np.random.normal(loc=0, scale=3, size=len(date_range))\n",
    "    \n",
    "    # Introduce anomalies (simulate electricity theft by reducing consumption)\n",
    "    anomaly_days = np.random.choice(range(len(date_range)), size=10, replace=False)\n",
    "    normal_usage[anomaly_days] *= np.random.uniform(0.1, 0.5, size=len(anomaly_days))  # Reduce consumption by 50-90%\n",
    "    \n",
    "    # Store data in dictionary\n",
    "    consumption_data[f'household_{i+1}'] = normal_usage\n",
    "\n",
    "# Simulate daily average temperatures with seasonal variation\n",
    "# Define temperature trends for each season: winter, spring, summer, fall\n",
    "# Using rough ranges: winter (-5 to 10°C), spring (5 to 20°C), summer (15 to 35°C), fall (5 to 20°C)\n",
    "temperatures = []\n",
    "for date in date_range:\n",
    "    month = date.month\n",
    "    if month in [12, 1, 2]:  # Winter months\n",
    "        avg_temp = np.random.uniform(-5, 10)\n",
    "    elif month in [3, 4, 5]:  # Spring months\n",
    "        avg_temp = np.random.uniform(5, 20)\n",
    "    elif month in [6, 7, 8]:  # Summer months\n",
    "        avg_temp = np.random.uniform(15, 35)\n",
    "    else:  # Fall months (September, October, November)\n",
    "        avg_temp = np.random.uniform(5, 20)\n",
    "    temperatures.append(avg_temp)\n",
    "\n",
    "# Create DataFrame for consumption data\n",
    "data = pd.DataFrame(consumption_data, index=date_range)\n",
    "\n",
    "# Add temperature data to the DataFrame\n",
    "data['avg_temperature'] = temperatures\n",
    "\n",
    "# Melt the DataFrame to get household and date as rows (instead of households as columns)\n",
    "melted_data = data.reset_index().melt(id_vars=['index', 'avg_temperature'], var_name='household_id', value_name='daily_consumption')\n",
    "\n",
    "# Rename the 'index' column to 'date' for clarity\n",
    "melted_data.rename(columns={'index': 'date'}, inplace=True)\n",
    "\n",
    "# Create household metadata DataFrame (for household size)\n",
    "metadata = pd.DataFrame({\n",
    "    'household_id': [f'household_{i+1}' for i in range(n_households)],\n",
    "    'household_size': household_size,\n",
    "    'base_consumption': base_consumption\n",
    "})\n",
    "\n",
    "# Merge the melted consumption data with the metadata\n",
    "final_data = pd.merge(melted_data, metadata, on='household_id', how='left')\n",
    "\n",
    "# Save to a single CSV file\n",
    "final_data.to_csv('synthetic_household_energy_data_with_metadata.csv', index=False)\n",
    "\n",
    "print(\"Dataset with consumption data, temperature, and household metadata saved as 'synthetic_household_energy_data_with_metadata.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194cfd43-741a-4718-8980-ea8d253d3fc8",
   "metadata": {},
   "source": [
    "This code generates a year's worth of daily electricity consumption data for 10 households. Some anomalies (where consumption is unusually low) are injected randomly to simulate electricity theft.\n",
    "\n",
    "# 2. load and process the synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52abd97f-0d1a-425e-81ce-83beb541197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the synthetic dataset\n",
    "data = pd.read_csv('synthetic_household_energy_data_with_metadata.csv', parse_dates=['date'])\n",
    "\n",
    "# Check the data structure\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b32534-49fb-44b0-872e-f6afdcf9998c",
   "metadata": {},
   "source": [
    "# 3. Data preprocessing\n",
    "\n",
    "a. Aggregating Consumption Data\n",
    "You can aggregate the daily consumption data for each household into broader periods (e.g., weekly, monthly, or yearly averages) to capture general consumption patterns. Using too detailed data might lead to noisy clustering, while aggregated patterns may highlight broader trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5309e0b5-5113-46b8-a4dd-b4887634e04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating the daily consumption data into monthly averages\n",
    "monthly_data = data.groupby(['household_id', pd.Grouper(key='date', freq='M')]).mean()\n",
    "\n",
    "# Displaying the first few rows of the aggregated monthly data\n",
    "monthly_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c72155-2899-4c50-b4da-909b5cfb3b7e",
   "metadata": {},
   "source": [
    "b. Feature Scaling\n",
    "Since energy consumption might vary drastically across households, you’ll want to normalize the data. Standardizing the data will help make the clustering algorithm more sensitive to relative differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d30f9f8f-cfdb-433c-96bb-3f97df84f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop temperature data if clustering only on energy consumption\n",
    "scaled_data = monthly_data.drop(columns=['avg_temperature'], axis=1)\n",
    "\n",
    "# Normalize the data (scale each household's data)\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f05863-68b6-44c6-9c5f-fced81434284",
   "metadata": {},
   "source": [
    "# 4. Clustering the Households\n",
    "\n",
    "a. apply a clustering algorithm to identify households with similar consumption patterns.\n",
    "\n",
    "K-Means: Efficient for large datasets but requires specifying the number of clusters (k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684bd4c9-5429-499e-b1b9-4beed445100e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Elbow method to determine the number of clusters\n",
    "inertia = []\n",
    "K = range(1, 10)  # Check calusters from 1 to 9\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(scaled_data)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the elbow graph\n",
    "plt.plot(K, inertia, 'bx-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cd0846-e151-4e1f-a76f-f1746f417372",
   "metadata": {},
   "source": [
    "b. Cluster the Households\n",
    "Once you've selected the number of clusters, apply the K-Means algorithm to segment the households."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bcef0f9-cfda-47a8-9485-fece7ba06d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming the optimal number of clusters is determined as k=3 from the elbow method\n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "monthly_data['cluster'] = kmeans.fit_predict(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8763754-d45d-43b0-ae38-7c3f47ecaed5",
   "metadata": {},
   "source": [
    "c. Merge the clusters back with the original dataset: If you want to visualize the clusters in the daily data (final_data), you should map the clusters from monthly_data back to the daily data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d990e34-974f-433d-980c-d3d85f3a6367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to merge the monthly clusters with the original data\n",
    "monthly_data.reset_index(inplace=True)\n",
    "\n",
    "# Merge clusters with original daily data on 'household_id' and 'date' (ensure date aligns with monthly grouping)\n",
    "final_data = pd.merge(final_data, monthly_data[['household_id', 'date', 'cluster']], \n",
    "                      on=['household_id', 'date'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780faf6f-d310-4192-a3f3-a581b0b4a34f",
   "metadata": {},
   "source": [
    "# 5. Labeling the Data\n",
    "To detect electricity theft, you need to label or identify the anomalous consumption patterns within the dataset. Since you've already injected anomalies (reduced consumption on certain days to simulate theft), you can use this information to label those records.\n",
    "\n",
    "Label Creation:\n",
    "Add a binary column to flag potential theft cases (1 for theft, 0 for normal consumption). This could be based on the conditions you introduced for reducing consumption (e.g., a significant drop in consumption compared to normal usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e86fda4a-c98e-4dae-a843-8b21a584e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label theft anomalies (assuming that days with a significant drop in consumption are thefts)\n",
    "final_data['theft_label'] = final_data.apply(\n",
    "    lambda row: 1 if row['daily_consumption'] < 0.5 * row['base_consumption'] else 0, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40abd6c-80d3-4a21-a0f0-4d9437844057",
   "metadata": {},
   "source": [
    "# 6. Feature Engineering\n",
    "\n",
    "To detect anomalies, we create an additional feature that can highlight abnormal behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "610701f4-cb75-404f-9b5e-c300699487c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a rolling mean and standard deviation for each household's consumption\n",
    "final_data['rolling_mean_7d'] = final_data.groupby('household_id')['daily_consumption'].transform(lambda x: x.rolling(window=7).mean())\n",
    "final_data['rolling_std_7d'] = final_data.groupby('household_id')['daily_consumption'].transform(lambda x: x.rolling(window=7).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190288ae-a19d-41e2-8530-d2e2b459cf9f",
   "metadata": {},
   "source": [
    "# 7. Modeling for Anomaly Detection \n",
    "\n",
    "Unsupervised Anomaly Detection\n",
    "\n",
    "   a.  Train the IsolationForest on monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec6fbd34-d097-49e2-953a-9ab8697a2fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an Isolation Forest on the scaled monthly data\n",
    "iso_forest = IsolationForest(contamination=0.01, random_state=42)  # 1% contamination (anomalies)\n",
    "monthly_data['anomaly'] = iso_forest.fit_predict(scaled_data)\n",
    "\n",
    "# Map 1 and -1 to binary values (1 = anomaly, 0 = normal)\n",
    "monthly_data['anomaly'] = monthly_data['anomaly'].map({1: 0, -1: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006720c1-b636-493b-b664-5e9878ffc170",
   "metadata": {},
   "source": [
    "b. Merge Anomalies with Daily Data\n",
    "\n",
    "   To assign the detected monthly anomalies to the daily data, merge monthly_data back to final_data. We'll use household_id and the month of the date column as keys for the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6fe19f-3e79-4acb-9373-7e1614773390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract month and year from the date to align with monthly data\n",
    "monthly_data.reset_index(inplace=True)\n",
    "monthly_data['month'] = monthly_data['date'].dt.to_period('M')  # Convert to monthly period\n",
    "\n",
    "# Create a similar 'month' column in final_data to facilitate merging\n",
    "final_data['month'] = final_data['date'].dt.to_period('M')\n",
    "\n",
    "# Merge the monthly anomaly detection results back to the daily final_data\n",
    "final_data = pd.merge(final_data, monthly_data[['household_id', 'month', 'anomaly']], \n",
    "                      on=['household_id', 'month'], how='left')\n",
    "\n",
    "# Check the merged data with anomalies\n",
    "print(final_data[final_data['anomaly'] == 1].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed980e7-7a42-4d92-b4f3-e7c0d0b9f963",
   "metadata": {},
   "source": [
    "b. Supervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba1989a-7e7e-4790-8d78-0d7e0c9d3b8c",
   "metadata": {},
   "source": [
    "Random Forest Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c68b26-798a-4949-9f43-5de0f85821e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "\n",
    "# Define features and labels  \n",
    "X = final_data.drop(columns=['theft_label', 'household_id', 'date'])  \n",
    "y = final_data['theft_label']  \n",
    "\n",
    "# Split the data into training and testing sets  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
    "\n",
    "# Train the Random Forest  \n",
    "rf_clf = RandomForestClassifier(random_state=42)  \n",
    "rf_clf.fit(X_train, y_train)  \n",
    "\n",
    "# Make predictions  \n",
    "y_pred = rf_clf.predict(X_test)  \n",
    "\n",
    "# Evaluate the model  \n",
    "print(classification_report(y_test, y_pred))  \n",
    "\n",
    "# Plot the confusion matrix  \n",
    "cm = confusion_matrix(y_test, y_pred)  \n",
    "plt.figure(figsize=(8, 6))  \n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')  \n",
    "plt.title('Confusion Matrix')  \n",
    "plt.xlabel('Predicted')  \n",
    "plt.ylabel('True')  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649f76ec-d6a9-4796-9b9d-36d270fd97e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95db8136-7d94-4dc3-9458-d00e37be4def",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
